{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from posixpath import join\n",
    "import paramiko\n",
    "from scp import SCPClient\n",
    "import subprocess\n",
    "\n",
    "run_id = 0\n",
    "jar_name = 'letter-frequency-1.0-SNAPSHOT.jar'\n",
    "# connect with ssh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First analyze the letter frequencies in books across the eras. As performance is not relevant now, the job will be executed with 1 reducer, using combiner.\n",
    "We hypotize that the input files are already in the input folder in hdfs, in /user/hadoop/letter_analysis/input/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh = paramiko.SSHClient()\n",
    "ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "ssh.connect('10.1.1.124', username='hadoop', password='ubuntu')\n",
    "root_dir = '/user/hadoop/historical_analysis'\n",
    "\n",
    "# run the job for each file\n",
    "for txt_file in os.listdir('../resources/historical_analysis/input'):\n",
    "    print(f'Processing {txt_file}')\n",
    "    input_dir = join(root_dir, 'input', txt_file)\n",
    "    output_dir = join(root_dir, f'output_{run_id}', txt_file.split('.')[0])\n",
    "    stdin, stdout, stderr = ssh.exec_command(f'/opt/hadoop/bin/hadoop jar {jar_name} it.unipi.cloud.MapReduceApp {input_dir} {output_dir}/count {output_dir}/freq 1 inmappercombiner')\n",
    "    print(stderr.read().decode('utf-8'))\n",
    "    # print(stdout.read().decode('utf-8'))\n",
    "\n",
    "# copy the output to local\n",
    "stdin, stdout, stderr = ssh.exec_command(f'/opt/hadoop/bin/hadoop fs -copyToLocal /user/hadoop/historical_analysis/output_{run_id} .')\n",
    "print(stderr.read().decode('utf-8'))\n",
    "\n",
    "if not os.path.exists(f'../resources/historical_analysis/output_{run_id}'):\n",
    "    os.mkdir(f'../resources/historical_analysis/output_{run_id}')\n",
    "    \n",
    "scp = SCPClient(ssh.get_transport())\n",
    "scp.get(f'output_{run_id}', '../resources/historical_analysis/', recursive=True)\n",
    "scp.close()\n",
    "ssh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that we can analyze the output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] Impossibile trovare il percorso specificato: '/user/hadoop/historical_analysis/output'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m output_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/user/hadoop/historical_analysis/output\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Get the list of files in the output folder\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m files \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(output_folder)\n\u001b[0;32m     11\u001b[0m letter_over_years \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Process each file\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] Impossibile trovare il percorso specificato: '/user/hadoop/historical_analysis/output'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "output_folder = '/user/hadoop/historical_analysis/output'\n",
    "\n",
    "# Get the list of files in the output folder\n",
    "files = os.listdir(output_folder)\n",
    "\n",
    "letter_over_years = []\n",
    "\n",
    "# Process each file\n",
    "for file in files:\n",
    "    # Extract the year from the file name\n",
    "    year = file.split('_')[1].split('.')[0]\n",
    "    \n",
    "    # Read the file as a pandas DataFrame\n",
    "    df = pd.read_csv(os.path.join(output_folder, file), names=['Letter', 'Frequency'])\n",
    "    letter_over_years.append((year, df['Frequency'].where(df['Letter'] == 'a')))\n",
    "    \n",
    "# Plot the trend of letter frequencies over time\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for year, df in letter_over_years.items():\n",
    "    ax.plot(df['Letter'], df['Relative Frequency'], label=year)\n",
    "\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Relative Frequency')\n",
    "ax.set_title('Trend of letter \\'a\\' frequency over time')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
