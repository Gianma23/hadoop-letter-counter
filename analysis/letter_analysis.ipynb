{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import paramiko\n",
    "\n",
    "# connect with ssh\n",
    "ssh = paramiko.SSHClient()\n",
    "ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "ssh.connect('10.1.1.124', username='hadoop', password='ubuntu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First analyze the letter frequencies in books across the eras. As performance is not relevant now, the job will be executed with 1 reducer, using combiner.\n",
    "We hypotize that the input files are already in the input folder in hdfs, in /user/hadoop/letter_analysis/input/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/user/hadoop/historical_analysis'\n",
    "jar_name = 'letter-counter-1.0-SNAPSHOT.jar'\n",
    "input_dir = os.path.join(root_dir, 'input')\n",
    "output_dir = os.path.join(root_dir, 'output')\n",
    "# remove old outputs\n",
    "ssh.exec_command('hadoop fs -rm -r ' + output_dir)\n",
    "\n",
    "# run the job for each file\n",
    "for txt_file in os.listdir('../resources/historical_analysis'):\n",
    "    ssh.exec_command('hadoop jar ' + jar_name + ' ' +\n",
    "                input_dir + '/' + txt_file + ' ' + \n",
    "                output_dir + ' ' +\n",
    "                '1 combiner' # number of reducers and method to use\n",
    "              )\n",
    "    # copy the output to local\n",
    "    os.system('hadoop fs -copyToLocal ' + output_dir + '/' + txt_file + ' ../resources/historical_analysis/output')\n",
    "\n",
    "ssh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that we can analyze the output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "output_folder = '/user/hadoop/historical_analysis/output'\n",
    "\n",
    "# Get the list of files in the output folder\n",
    "files = os.listdir(output_folder)\n",
    "\n",
    "letter_over_years = []\n",
    "\n",
    "# Process each file\n",
    "for file in files:\n",
    "    # Extract the year from the file name\n",
    "    year = file.split('_')[1].split('.')[0]\n",
    "    \n",
    "    # Read the file as a pandas DataFrame\n",
    "    df = pd.read_csv(os.path.join(output_folder, file), names=['Letter', 'Frequency'])\n",
    "    letter_over_years.append((year, df['Frequency'].where(df['Letter'] == 'a')))\n",
    "    \n",
    "# Plot the trend of letter frequencies over time\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for year, df in letter_over_years.items():\n",
    "    ax.plot(df['Letter'], df['Relative Frequency'], label=year)\n",
    "\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Relative Frequency')\n",
    "ax.set_title('Trend of letter \\'a\\' frequency over time')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
