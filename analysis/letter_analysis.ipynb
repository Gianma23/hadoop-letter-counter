{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utente\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from posixpath import join\n",
    "import paramiko\n",
    "from scp import SCPClient\n",
    "import shutil\n",
    "\n",
    "run_id = 0\n",
    "jar_name = 'letter-frequency-1.0-SNAPSHOT.jar'\n",
    "# connect with ssh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First analyze the letter frequencies in books across the eras. As performance is not relevant now, the job will be executed with 1 reducer, using combiner.\n",
    "We hypotize that the input files are already in the input folder in hdfs, in /user/hadoop/letter_analysis/input/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" ssh = paramiko.SSHClient()\\nssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\\nssh.connect('10.1.1.124', username='hadoop', password='ubuntu')\\nroot_dir = '/user/hadoop/historical_analysis'\\n\\n# run the job for each file\\nfor txt_file in os.listdir('../resources/historical_analysis/input'):\\n    print(f'Processing {txt_file}')\\n    input_dir = join(root_dir, 'input', txt_file)\\n    output_dir = join(root_dir, f'output_{run_id}', txt_file.split('.')[0])\\n    stdin, stdout, stderr = ssh.exec_command(f'/opt/hadoop/bin/hadoop jar {jar_name} it.unipi.cloud.MapReduceApp '\\n                                             f'{input_dir} {output_dir}/count {output_dir}/freq 1 inmappercombiner')\\n    print(stderr.read().decode('utf-8'))\\n    print(stdout.read().decode('utf-8'))\\n\\n# copy the output to local\\nstdin, stdout, stderr = ssh.exec_command(f'/opt/hadoop/bin/hadoop fs -copyToLocal /user/hadoop/historical_analysis/output_{run_id} .')\\nprint(stderr.read().decode('utf-8'))\\n\\nif not os.path.exists(f'../resources/historical_analysis/output_{run_id}'):\\n    os.mkdir(f'../resources/historical_analysis/output_{run_id}')\\n    \\nscp = SCPClient(ssh.get_transport())\\nscp.get(f'output_{run_id}', '../resources/historical_analysis/', recursive=True)\\n\\n# remove the output\\nssh.exec_command(f'rm -r output_{run_id}')\\nscp.close()\\nssh.close() \""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" ssh = paramiko.SSHClient()\n",
    "ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "ssh.connect('10.1.1.124', username='hadoop', password='ubuntu')\n",
    "root_dir = '/user/hadoop/historical_analysis'\n",
    "\n",
    "# run the job for each file\n",
    "for txt_file in os.listdir('../resources/historical_analysis/input'):\n",
    "    print(f'Processing {txt_file}')\n",
    "    input_dir = join(root_dir, 'input', txt_file)\n",
    "    output_dir = join(root_dir, f'output_{run_id}', txt_file.split('.')[0])\n",
    "    stdin, stdout, stderr = ssh.exec_command(f'/opt/hadoop/bin/hadoop jar {jar_name} it.unipi.cloud.MapReduceApp '\n",
    "                                             f'{input_dir} {output_dir}/count {output_dir}/freq 1 inmappercombiner')\n",
    "    print(stderr.read().decode('utf-8'))\n",
    "    print(stdout.read().decode('utf-8'))\n",
    "\n",
    "# copy the output to local\n",
    "stdin, stdout, stderr = ssh.exec_command(f'/opt/hadoop/bin/hadoop fs -copyToLocal /user/hadoop/historical_analysis/output_{run_id} .')\n",
    "print(stderr.read().decode('utf-8'))\n",
    "\n",
    "if not os.path.exists(f'../resources/historical_analysis/output_{run_id}'):\n",
    "    os.mkdir(f'../resources/historical_analysis/output_{run_id}')\n",
    "    \n",
    "scp = SCPClient(ssh.get_transport())\n",
    "scp.get(f'output_{run_id}', '../resources/historical_analysis/', recursive=True)\n",
    "\n",
    "# remove the output\n",
    "ssh.exec_command(f'rm -r output_{run_id}')\n",
    "scp.close()\n",
    "ssh.close() \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that we can analyze the output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" import os\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\nrun_id = 0\\n\\ndirectory = f'../resources/historical_analysis/output_{run_id}'\\nprint(directory)\\n\\nyears = []\\nfor filename in os.listdir(directory):\\n    year = filename.split('.')[0]\\n    years.append(year)\\n\\ndf = pd.DataFrame(index=years, columns=[])\\n\\nfor year in years:\\n    freq_per_year = []\\n    filepath = f'{directory}/{year}/freq/part-r-00000'\\n    with open(filepath, 'r') as f:\\n        for line in f:\\n            letter, freq = line.strip().split('\\t')\\n            freq = float(freq)\\n            df.loc[year, letter] = freq\\nprint(df)\\n\\ndf.plot( kind='bar', stacked=True, title='Letter frequency over the years', figsize=(20, 10)) \""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "run_id = 0\n",
    "\n",
    "directory = f'../resources/historical_analysis/output_{run_id}'\n",
    "print(directory)\n",
    "\n",
    "years = []\n",
    "for filename in os.listdir(directory):\n",
    "    year = filename.split('.')[0]\n",
    "    years.append(year)\n",
    "\n",
    "df = pd.DataFrame(index=years, columns=[])\n",
    "\n",
    "for year in years:\n",
    "    freq_per_year = []\n",
    "    filepath = f'{directory}/{year}/freq/part-r-00000'\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            letter, freq = line.strip().split('\\t')\n",
    "            freq = float(freq)\n",
    "            df.loc[year, letter] = freq\n",
    "print(df)\n",
    "\n",
    "df.plot( kind='bar', stacked=True, title='Letter frequency over the years', figsize=(20, 10)) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "performance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' import string\\nimport random\\n# Dimensioni dei file in bytes\\nsizes = [10 * 1024, 1 * 1024 * 1024, 100 * 1024 * 1024, 1 * 1024 * 1024 * 1024]\\nchunk_size = 100  # Dimensione del chunk in bytes\\n\\n# Nomi dei file\\nfile_dir = \"../resources/performance_analysis/input\"\\nfilenames = [os.path.join(file_dir, \"file_10KB.txt\"), \\n             os.path.join(file_dir, \"file_1MB.txt\"), \\n             os.path.join(file_dir, \"file_100MB.txt\"), \\n             os.path.join(file_dir, \"file_1GB.txt\")]\\n\\n# Genera una stringa di lettere casuali\\ndef generate_random_string(length):\\n    letters = string.ascii_letters\\n    return \\'\\'.join(random.choice(letters) for i in range(length))\\n\\nfor size, filename in zip(sizes, filenames):\\n    print(f\"Generating {filename}...\")\\n    with open(filename, \\'w\\') as f:\\n        for _ in range(size // chunk_size):\\n            f.write(generate_random_string(chunk_size))\\n            f.write(\\'\\n\\')\\n        remaining = size % chunk_size\\n        if remaining:\\n            f.write(generate_random_string(remaining))\\n    print(f\"File {filename} generated successfully.\") '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" import string\n",
    "import random\n",
    "# Dimensioni dei file in bytes\n",
    "sizes = [10 * 1024, 1 * 1024 * 1024, 100 * 1024 * 1024, 1 * 1024 * 1024 * 1024]\n",
    "chunk_size = 100  # Dimensione del chunk in bytes\n",
    "\n",
    "# Nomi dei file\n",
    "file_dir = \"../resources/performance_analysis/input\"\n",
    "filenames = [os.path.join(file_dir, \"file_10KB.txt\"), \n",
    "             os.path.join(file_dir, \"file_1MB.txt\"), \n",
    "             os.path.join(file_dir, \"file_100MB.txt\"), \n",
    "             os.path.join(file_dir, \"file_1GB.txt\")]\n",
    "\n",
    "# Genera una stringa di lettere casuali\n",
    "def generate_random_string(length):\n",
    "    letters = string.ascii_letters\n",
    "    return ''.join(random.choice(letters) for i in range(length))\n",
    "\n",
    "for size, filename in zip(sizes, filenames):\n",
    "    print(f\"Generating {filename}...\")\n",
    "    with open(filename, 'w') as f:\n",
    "        for _ in range(size // chunk_size):\n",
    "            f.write(generate_random_string(chunk_size))\n",
    "            f.write('\\n')\n",
    "        remaining = size % chunk_size\n",
    "        if remaining:\n",
    "            f.write(generate_random_string(remaining))\n",
    "    print(f\"File {filename} generated successfully.\") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 10KB.txt with combiner and 1 reducers\n",
      "\n",
      "2024-06-19 23:18:49,107 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2024-06-19 23:18:49,239 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "\n",
      "\n",
      "\n",
      "Processing 10KB.txt with combiner and 2 reducers\n"
     ]
    }
   ],
   "source": [
    "ssh = paramiko.SSHClient()\n",
    "ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "ssh.connect('10.1.1.124', username='hadoop', password='ubuntu')\n",
    "root_dir = '/user/hadoop/performance_analysis'\n",
    "methods = ['combiner', 'inmappercombiner']\n",
    "n_reducers = 3\n",
    "\n",
    "\n",
    "# try all the combinations of methods and reducers\n",
    "for method in methods:\n",
    "    for i in range(1, n_reducers+1):\n",
    "        for txt_file in os.listdir('../resources/performance_analysis/input'):\n",
    "            print(f'Processing {txt_file} with {method} and {i} reducers')\n",
    "            input_dir = join(root_dir, 'input', txt_file)\n",
    "            output_dir = join(root_dir, f'output_{run_id}_{method}_{i}', txt_file.split('.')[0])\n",
    "            ssh.exec_command(f'mkdir -p output_{run_id}_{method}_{i}/{txt_file.split(\".\")[0]}')\n",
    "\n",
    "            stdin, stdout, stderr = ssh.exec_command(f'/opt/hadoop/bin/hadoop jar {jar_name} it.unipi.cloud.MapReduceApp '\n",
    "                                                     f'{input_dir} {output_dir}/count {output_dir}/freq {i} {method} '\n",
    "                                                     f'> log.txt 2>&1')\n",
    "            print(stderr.read().decode('utf-8'))\n",
    "            \n",
    "            stdin, stdout, stderr = ssh.exec_command(f'/opt/hadoop/bin/hadoop fs -copyToLocal {output_dir} output_{run_id}_{method}_{i}/{txt_file.split(\".\")[0]}' )\n",
    "            print(stderr.read().decode('utf-8'))\n",
    "            stdin, stdout, stderr = ssh.exec_command(f'mv log.txt output_{run_id}_{method}_{i}/{txt_file.split(\".\")[0]}/log.txt')\n",
    "            print(stderr.read().decode('utf-8'))\n",
    "            \n",
    "        # create the output directory in project resources\n",
    "        if not os.path.exists(f'../resources/performance_analysis/output_{run_id}_{method}_{i}'):\n",
    "            os.mkdir(f'../resources/performance_analysis/output_{run_id}_{method}_{i}')\n",
    "\n",
    "        # move the output to local machine\n",
    "        scp = SCPClient(ssh.get_transport())\n",
    "        scp.get(f'output_{run_id}_{method}_{i}', '../resources/performance_analysis/', recursive=True)\n",
    "        \n",
    "        # remove the output directory from virtual machine\n",
    "        stdin, stdout, stderr = ssh.exec_command(f'rm -r output_{run_id}_{method}_{i}')\n",
    "        print(stderr.read().decode('utf-8'))\n",
    "\n",
    "scp.close()\n",
    "ssh.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
