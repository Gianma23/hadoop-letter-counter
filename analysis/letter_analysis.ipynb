{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from posixpath import join\n",
    "import paramiko\n",
    "from scp import SCPClient\n",
    "import subprocess\n",
    "\n",
    "run_id = 19\n",
    "jar_name = 'letter-frequency-1.0-SNAPSHOT.jar'\n",
    "# connect with ssh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First analyze the letter frequencies in books across the eras. As performance is not relevant now, the job will be executed with 1 reducer, using combiner.\n",
    "We hypotize that the input files are already in the input folder in hdfs, in /user/hadoop/letter_analysis/input/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1294.txt\n",
      "2024-06-18 08:39:46,560 INFO client.RMProxy: Connecting to ResourceManager at hadoop-namenode/10.1.1.124:8032\n",
      "2024-06-18 08:39:47,102 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1716534983775_0173\n",
      "2024-06-18 08:39:47,201 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2024-06-18 08:39:47,360 WARN hdfs.DataStreamer: Caught exception\n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1257)\n",
      "\tat java.lang.Thread.join(Thread.java:1331)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n",
      "2024-06-18 08:39:47,420 INFO input.FileInputFormat: Total input files to process : 1\n",
      "2024-06-18 08:39:47,471 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2024-06-18 08:39:47,523 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2024-06-18 08:39:47,535 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "2024-06-18 08:39:47,653 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2024-06-18 08:39:47,686 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1716534983775_0173\n",
      "2024-06-18 08:39:47,687 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2024-06-18 08:39:47,887 INFO conf.Configuration: resource-types.xml not found\n",
      "2024-06-18 08:39:47,887 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2024-06-18 08:39:47,966 INFO impl.YarnClientImpl: Submitted application application_1716534983775_0173\n",
      "2024-06-18 08:39:48,038 INFO mapreduce.Job: The url to track the job: http://hadoop-namenode:8088/proxy/application_1716534983775_0173/\n",
      "2024-06-18 08:39:48,039 INFO mapreduce.Job: Running job: job_1716534983775_0173\n",
      "2024-06-18 08:39:55,195 INFO mapreduce.Job: Job job_1716534983775_0173 running in uber mode : false\n",
      "2024-06-18 08:39:55,198 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2024-06-18 08:39:59,291 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2024-06-18 08:40:05,392 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2024-06-18 08:40:06,421 INFO mapreduce.Job: Job job_1716534983775_0173 completed successfully\n",
      "2024-06-18 08:40:06,544 INFO mapreduce.Job: Counters: 53\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=22\n",
      "\t\tFILE: Number of bytes written=435341\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=145109\n",
      "\t\tHDFS: Number of bytes written=13\n",
      "\t\tHDFS: Number of read operations=8\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5494\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5832\n",
      "\t\tTotal time spent by all map tasks (ms)=2747\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2916\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=2747\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2916\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=703232\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=746496\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2791\n",
      "\t\tMap output records=1\n",
      "\t\tMap output bytes=14\n",
      "\t\tMap output materialized bytes=22\n",
      "\t\tInput split bytes=139\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=22\n",
      "\t\tReduce input records=1\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=2\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=174\n",
      "\t\tCPU time spent (ms)=1350\n",
      "\t\tPhysical memory (bytes) snapshot=444170240\n",
      "\t\tVirtual memory (bytes) snapshot=3736608768\n",
      "\t\tTotal committed heap usage (bytes)=331350016\n",
      "\t\tPeak Map Physical memory (bytes)=273924096\n",
      "\t\tPeak Map Virtual memory (bytes)=1864007680\n",
      "\t\tPeak Reduce Physical memory (bytes)=170246144\n",
      "\t\tPeak Reduce Virtual memory (bytes)=1872601088\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=144970\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=13\n",
      "2024-06-18 08:40:06,597 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2024-06-18 08:40:06,628 INFO client.RMProxy: Connecting to ResourceManager at hadoop-namenode/10.1.1.124:8032\n",
      "2024-06-18 08:40:06,647 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1716534983775_0174\n",
      "2024-06-18 08:40:06,662 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2024-06-18 08:40:06,688 INFO input.FileInputFormat: Total input files to process : 1\n",
      "2024-06-18 08:40:06,704 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2024-06-18 08:40:06,766 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2024-06-18 08:40:06,782 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "2024-06-18 08:40:06,807 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2024-06-18 08:40:06,830 WARN hdfs.DataStreamer: Caught exception\n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1257)\n",
      "\tat java.lang.Thread.join(Thread.java:1331)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n",
      "2024-06-18 08:40:06,831 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1716534983775_0174\n",
      "2024-06-18 08:40:06,831 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2024-06-18 08:40:06,860 INFO impl.YarnClientImpl: Submitted application application_1716534983775_0174\n",
      "2024-06-18 08:40:06,864 INFO mapreduce.Job: The url to track the job: http://hadoop-namenode:8088/proxy/application_1716534983775_0174/\n",
      "2024-06-18 08:40:06,864 INFO mapreduce.Job: Running job: job_1716534983775_0174\n",
      "2024-06-18 08:40:18,011 INFO mapreduce.Job: Job job_1716534983775_0174 running in uber mode : false\n",
      "2024-06-18 08:40:18,012 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2024-06-18 08:40:23,070 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2024-06-18 08:40:29,132 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2024-06-18 08:40:29,154 INFO mapreduce.Job: Job job_1716534983775_0174 completed successfully\n",
      "2024-06-18 08:40:29,230 INFO mapreduce.Job: Counters: 53\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=116075\n",
      "\t\tFILE: Number of bytes written=668481\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=145109\n",
      "\t\tHDFS: Number of bytes written=600\n",
      "\t\tHDFS: Number of read operations=8\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6520\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5896\n",
      "\t\tTotal time spent by all map tasks (ms)=3260\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2948\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3260\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2948\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=834560\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=754688\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2791\n",
      "\t\tMap output records=9644\n",
      "\t\tMap output bytes=96781\n",
      "\t\tMap output materialized bytes=116075\n",
      "\t\tInput split bytes=139\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=26\n",
      "\t\tReduce shuffle bytes=116075\n",
      "\t\tReduce input records=9644\n",
      "\t\tReduce output records=26\n",
      "\t\tSpilled Records=19288\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=185\n",
      "\t\tCPU time spent (ms)=2260\n",
      "\t\tPhysical memory (bytes) snapshot=450310144\n",
      "\t\tVirtual memory (bytes) snapshot=3737247744\n",
      "\t\tTotal committed heap usage (bytes)=321388544\n",
      "\t\tPeak Map Physical memory (bytes)=277090304\n",
      "\t\tPeak Map Virtual memory (bytes)=1864253440\n",
      "\t\tPeak Reduce Physical memory (bytes)=173219840\n",
      "\t\tPeak Reduce Virtual memory (bytes)=1872994304\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=144970\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=600\n",
      "\n",
      "\n",
      "2024-06-18 08:40:31,434 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2024-06-18 08:40:31,562 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssh = paramiko.SSHClient()\n",
    "ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "ssh.connect('10.1.1.124', username='hadoop', password='ubuntu')\n",
    "root_dir = '/user/hadoop/historical_analysis'\n",
    "\n",
    "# run the job for each file\n",
    "for txt_file in os.listdir('../resources/historical_analysis/input'):\n",
    "    print(f'Processing {txt_file}')\n",
    "    input_dir = join(root_dir, 'input', txt_file)\n",
    "    output_dir = join(root_dir, f'output_{run_id}', txt_file.split('.')[0])\n",
    "    stdin, stdout, stderr = ssh.exec_command(f'/opt/hadoop/bin/hadoop jar {jar_name} it.unipi.cloud.MapReduceApp {input_dir} {output_dir}/count {output_dir}/freq 1 inmappercombiner')\n",
    "    print(stderr.read().decode('utf-8'))\n",
    "    print(stdout.read().decode('utf-8'))\n",
    "\n",
    "# copy the output to local\n",
    "stdin, stdout, stderr = ssh.exec_command(f'/opt/hadoop/bin/hadoop fs -copyToLocal /user/hadoop/historical_analysis/output_{run_id} .')\n",
    "print(stderr.read().decode('utf-8'))\n",
    "\n",
    "if not os.path.exists(f'../resources/historical_analysis/output_{run_id}'):\n",
    "    os.mkdir(f'../resources/historical_analysis/output_{run_id}')\n",
    "    \n",
    "scp = SCPClient(ssh.get_transport())\n",
    "scp.get(f'output_{run_id}', '../resources/historical_analysis/', recursive=True)\n",
    "scp.close()\n",
    "ssh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that we can analyze the output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] Impossibile trovare il percorso specificato: '/user/hadoop/historical_analysis/output'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m output_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/user/hadoop/historical_analysis/output\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Get the list of files in the output folder\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m files \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(output_folder)\n\u001b[0;32m     11\u001b[0m letter_over_years \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Process each file\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] Impossibile trovare il percorso specificato: '/user/hadoop/historical_analysis/output'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "output_folder = '/user/hadoop/historical_analysis/output'\n",
    "\n",
    "# Get the list of files in the output folder\n",
    "files = os.listdir(output_folder)\n",
    "\n",
    "letter_over_years = []\n",
    "\n",
    "# Process each file\n",
    "for file in files:\n",
    "    # Extract the year from the file name\n",
    "    year = file.split('_')[1].split('.')[0]\n",
    "    \n",
    "    # Read the file as a pandas DataFrame\n",
    "    df = pd.read_csv(os.path.join(output_folder, file), names=['Letter', 'Frequency'])\n",
    "    letter_over_years.append((year, df['Frequency'].where(df['Letter'] == 'a')))\n",
    "    \n",
    "# Plot the trend of letter frequencies over time\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for year, df in letter_over_years.items():\n",
    "    ax.plot(df['Letter'], df['Relative Frequency'], label=year)\n",
    "\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Relative Frequency')\n",
    "ax.set_title('Trend of letter \\'a\\' frequency over time')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
