{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from posixpath import join\n",
    "import paramiko\n",
    "from scp import SCPClient\n",
    "import subprocess\n",
    "\n",
    "run_id = 20\n",
    "jar_name = 'letter-frequency-1.0-SNAPSHOT.jar'\n",
    "# connect with ssh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First analyze the letter frequencies in books across the eras. As performance is not relevant now, the job will be executed with 1 reducer, using combiner.\n",
    "We hypotize that the input files are already in the input folder in hdfs, in /user/hadoop/letter_analysis/input/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1294.txt\n",
      "2024-06-18 09:24:56,767 INFO client.RMProxy: Connecting to ResourceManager at hadoop-namenode/10.1.1.124:8032\n",
      "2024-06-18 09:24:57,304 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1716534983775_0175\n",
      "2024-06-18 09:24:57,438 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2024-06-18 09:24:57,646 INFO input.FileInputFormat: Total input files to process : 1\n",
      "2024-06-18 09:24:57,692 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2024-06-18 09:24:57,743 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2024-06-18 09:24:57,760 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "2024-06-18 09:24:57,906 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2024-06-18 09:24:57,939 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1716534983775_0175\n",
      "2024-06-18 09:24:57,939 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2024-06-18 09:24:58,147 INFO conf.Configuration: resource-types.xml not found\n",
      "2024-06-18 09:24:58,147 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2024-06-18 09:24:58,270 INFO impl.YarnClientImpl: Submitted application application_1716534983775_0175\n",
      "2024-06-18 09:24:58,345 INFO mapreduce.Job: The url to track the job: http://hadoop-namenode:8088/proxy/application_1716534983775_0175/\n",
      "2024-06-18 09:24:58,347 INFO mapreduce.Job: Running job: job_1716534983775_0175\n",
      "2024-06-18 09:25:04,589 INFO mapreduce.Job: Job job_1716534983775_0175 running in uber mode : false\n",
      "2024-06-18 09:25:04,592 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2024-06-18 09:25:10,704 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2024-06-18 09:25:15,763 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2024-06-18 09:25:15,783 INFO mapreduce.Job: Job job_1716534983775_0175 completed successfully\n",
      "2024-06-18 09:25:15,907 INFO mapreduce.Job: Counters: 53\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=22\n",
      "\t\tFILE: Number of bytes written=435373\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=145109\n",
      "\t\tHDFS: Number of bytes written=13\n",
      "\t\tHDFS: Number of read operations=8\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6322\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5796\n",
      "\t\tTotal time spent by all map tasks (ms)=3161\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2898\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3161\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2898\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=809216\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=741888\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2791\n",
      "\t\tMap output records=1\n",
      "\t\tMap output bytes=14\n",
      "\t\tMap output materialized bytes=22\n",
      "\t\tInput split bytes=139\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=22\n",
      "\t\tReduce input records=1\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=2\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=174\n",
      "\t\tCPU time spent (ms)=1390\n",
      "\t\tPhysical memory (bytes) snapshot=445603840\n",
      "\t\tVirtual memory (bytes) snapshot=3735556096\n",
      "\t\tTotal committed heap usage (bytes)=326631424\n",
      "\t\tPeak Map Physical memory (bytes)=273956864\n",
      "\t\tPeak Map Virtual memory (bytes)=1863405568\n",
      "\t\tPeak Reduce Physical memory (bytes)=171646976\n",
      "\t\tPeak Reduce Virtual memory (bytes)=1872150528\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=144970\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=13\n",
      "2024-06-18 09:25:15,961 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2024-06-18 09:25:15,991 INFO client.RMProxy: Connecting to ResourceManager at hadoop-namenode/10.1.1.124:8032\n",
      "2024-06-18 09:25:16,013 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1716534983775_0176\n",
      "2024-06-18 09:25:16,026 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2024-06-18 09:25:16,055 INFO input.FileInputFormat: Total input files to process : 1\n",
      "2024-06-18 09:25:16,068 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2024-06-18 09:25:16,098 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2024-06-18 09:25:16,112 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "2024-06-18 09:25:16,136 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2024-06-18 09:25:16,166 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1716534983775_0176\n",
      "2024-06-18 09:25:16,166 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2024-06-18 09:25:16,188 INFO impl.YarnClientImpl: Submitted application application_1716534983775_0176\n",
      "2024-06-18 09:25:16,196 INFO mapreduce.Job: The url to track the job: http://hadoop-namenode:8088/proxy/application_1716534983775_0176/\n",
      "2024-06-18 09:25:16,196 INFO mapreduce.Job: Running job: job_1716534983775_0176\n",
      "2024-06-18 09:25:28,368 INFO mapreduce.Job: Job job_1716534983775_0176 running in uber mode : false\n",
      "2024-06-18 09:25:28,370 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2024-06-18 09:25:33,441 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2024-06-18 09:25:39,512 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2024-06-18 09:25:39,524 INFO mapreduce.Job: Job job_1716534983775_0176 completed successfully\n",
      "2024-06-18 09:25:39,586 INFO mapreduce.Job: Counters: 53\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=116075\n",
      "\t\tFILE: Number of bytes written=668481\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=145109\n",
      "\t\tHDFS: Number of bytes written=600\n",
      "\t\tHDFS: Number of read operations=8\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6984\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=6244\n",
      "\t\tTotal time spent by all map tasks (ms)=3492\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3122\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3492\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3122\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=893952\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=799232\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2791\n",
      "\t\tMap output records=9644\n",
      "\t\tMap output bytes=96781\n",
      "\t\tMap output materialized bytes=116075\n",
      "\t\tInput split bytes=139\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=26\n",
      "\t\tReduce shuffle bytes=116075\n",
      "\t\tReduce input records=9644\n",
      "\t\tReduce output records=26\n",
      "\t\tSpilled Records=19288\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=185\n",
      "\t\tCPU time spent (ms)=2360\n",
      "\t\tPhysical memory (bytes) snapshot=444878848\n",
      "\t\tVirtual memory (bytes) snapshot=3736326144\n",
      "\t\tTotal committed heap usage (bytes)=318767104\n",
      "\t\tPeak Map Physical memory (bytes)=276250624\n",
      "\t\tPeak Map Virtual memory (bytes)=1863618560\n",
      "\t\tPeak Reduce Physical memory (bytes)=168628224\n",
      "\t\tPeak Reduce Virtual memory (bytes)=1872707584\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=144970\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=600\n",
      "\n",
      "\n",
      "2024-06-18 09:25:45,546 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2024-06-18 09:25:45,682 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssh = paramiko.SSHClient()\n",
    "ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "ssh.connect('10.1.1.124', username='hadoop', password='ubuntu')\n",
    "root_dir = '/user/hadoop/historical_analysis'\n",
    "\n",
    "# run the job for each file\n",
    "for txt_file in os.listdir('../resources/historical_analysis/input'):\n",
    "    print(f'Processing {txt_file}')\n",
    "    input_dir = join(root_dir, 'input', txt_file)\n",
    "    output_dir = join(root_dir, f'output_{run_id}', txt_file.split('.')[0])\n",
    "    stdin, stdout, stderr = ssh.exec_command(f'/opt/hadoop/bin/hadoop jar {jar_name} it.unipi.cloud.MapReduceApp {input_dir} {output_dir}/count {output_dir}/freq 1 inmappercombiner')\n",
    "    print(stderr.read().decode('utf-8'))\n",
    "    print(stdout.read().decode('utf-8'))\n",
    "\n",
    "# copy the output to local\n",
    "stdin, stdout, stderr = ssh.exec_command(f'/opt/hadoop/bin/hadoop fs -copyToLocal /user/hadoop/historical_analysis/output_{run_id} .')\n",
    "print(stderr.read().decode('utf-8'))\n",
    "\n",
    "if not os.path.exists(f'../resources/historical_analysis/output_{run_id}'):\n",
    "    os.mkdir(f'../resources/historical_analysis/output_{run_id}')\n",
    "    \n",
    "scp = SCPClient(ssh.get_transport())\n",
    "scp.get(f'output_{run_id}', '../resources/historical_analysis/', recursive=True)\n",
    "scp.close()\n",
    "ssh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that we can analyze the output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] Impossibile trovare il percorso specificato: '/user/hadoop/historical_analysis/output'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m output_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/user/hadoop/historical_analysis/output\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Get the list of files in the output folder\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m files \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(output_folder)\n\u001b[0;32m     11\u001b[0m letter_over_years \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Process each file\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] Impossibile trovare il percorso specificato: '/user/hadoop/historical_analysis/output'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "run_id = 0\n",
    "\n",
    "directory = f'../resources/historical_analysis/output_{run_id}'\n",
    "print(directory)\n",
    "\n",
    "years = []\n",
    "for filename in os.listdir(directory):\n",
    "    year = filename.split('.')[0]\n",
    "    years.append(year)\n",
    "\n",
    "# col = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "# df = pd.DataFrame(columns=col)\n",
    "\n",
    "df = pd.DataFrame(index=years, columns=[])\n",
    "\n",
    "for year in years:\n",
    "    freq_per_year = []\n",
    "    filepath = f'{directory}/{year}/freq/part-r-00000'\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            letter, freq = line.strip().split('\\t')\n",
    "            freq = float(freq)\n",
    "            # if (letter in col):\n",
    "                # df.loc[year, letter] = freq\n",
    "            df.loc[year, letter] = freq\n",
    "print(df)\n",
    "\n",
    "df.plot( kind='bar', stacked=True, title='Letter frequency over the years', figsize=(20, 10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
