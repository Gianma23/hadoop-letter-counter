{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import random\n",
    "import paramiko\n",
    "import pandas as pd\n",
    "from scp import SCPClient\n",
    "from posixpath import join\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "\n",
    "run_id = 3\n",
    "jar_name = 'letter-frequency-1.0-SNAPSHOT.jar'\n",
    "n_reducers = 3\n",
    "methods = ['combiner', 'inmappercombiner']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. generate text files with random letters for performance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file size in bytes\n",
    "sizes = [10 * 1024, 1 * 1024 * 1024, 100 * 1024 * 1024, 1 * 1024 * 1024 * 1024]\n",
    "chunk_size = 100  # Chunk size in bytes\n",
    "\n",
    "# file names\n",
    "file_dir = \"../resources/performance_analysis/input\"\n",
    "filenames = [os.path.join(file_dir, \"10KB.txt\"), \n",
    "             os.path.join(file_dir, \"1MB.txt\"), \n",
    "             os.path.join(file_dir, \"100MB.txt\"), \n",
    "             os.path.join(file_dir, \"1GB.txt\")]\n",
    "\n",
    "# generate a string of random letters\n",
    "def generate_random_string(length):\n",
    "    letters = string.ascii_letters\n",
    "    return ''.join(random.choice(letters) for i in range(length))\n",
    "\n",
    "for size, filename in zip(sizes, filenames):\n",
    "    print(f\"Generating {filename}...\")\n",
    "    with open(filename, 'w') as f:\n",
    "        for _ in range(size // chunk_size):\n",
    "            f.write(generate_random_string(chunk_size))\n",
    "            f.write('\\n')\n",
    "        remaining = size % chunk_size\n",
    "        if remaining:\n",
    "            f.write(generate_random_string(remaining))\n",
    "    print(f\"File {filename} generated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. for each combination of method and number of reducers, run the application for each text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh = paramiko.SSHClient()\n",
    "ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "ssh.connect('10.1.1.124', username='hadoop', password='ubuntu')\n",
    "root_dir = '/user/hadoop/performance_analysis'\n",
    "\n",
    "# try all the combinations of methods and reducers\n",
    "for method in methods:\n",
    "    for i in range(1, n_reducers+1):\n",
    "        for txt_file in os.listdir('../resources/performance_analysis/input'):\n",
    "            print(f'Processing {txt_file} with {method} and {i} reducers')\n",
    "            input_dir = join(root_dir, 'input', txt_file)\n",
    "            output_dir = join(root_dir, f'output_{run_id}_{method}_{i}', txt_file.split('.')[0])\n",
    "            ssh.exec_command(f'mkdir -p output_{run_id}_{method}_{i}/{txt_file.split(\".\")[0]}')\n",
    "\n",
    "            stdin, stdout, stderr = ssh.exec_command(f'/opt/hadoop/bin/hadoop jar {jar_name} it.unipi.cloud.MapReduceApp '\n",
    "                                                     f'{input_dir} {output_dir}/count {output_dir}/freq {i} {method} '\n",
    "                                                     f'> log.txt 2>&1')\n",
    "            print(stderr.read().decode('utf-8'))\n",
    "            \n",
    "            stdin, stdout, stderr = ssh.exec_command(f'/opt/hadoop/bin/hadoop fs -copyToLocal {output_dir} output_{run_id}_{method}_{i}/{txt_file.split(\".\")[0]}' )\n",
    "            print(stderr.read().decode('utf-8'))\n",
    "            stdin, stdout, stderr = ssh.exec_command(f'mv log.txt output_{run_id}_{method}_{i}/{txt_file.split(\".\")[0]}/log.txt')\n",
    "            print(stderr.read().decode('utf-8'))\n",
    "            \n",
    "        # create the output directory in project resources\n",
    "        if not os.path.exists(f'../resources/performance_analysis/output_{run_id}_{method}_{i}'):\n",
    "            os.mkdir(f'../resources/performance_analysis/output_{run_id}_{method}_{i}')\n",
    "\n",
    "        # move the output to local machine\n",
    "        scp = SCPClient(ssh.get_transport())\n",
    "        scp.get(f'output_{run_id}_{method}_{i}', '../resources/performance_analysis/', recursive=True)\n",
    "        \n",
    "        # remove the output directory from virtual machine\n",
    "        stdin, stdout, stderr = ssh.exec_command(f'rm -r output_{run_id}_{method}_{i}')\n",
    "        print(stderr.read().decode('utf-8'))\n",
    "\n",
    "scp.close()\n",
    "ssh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. plot the results for a specific list of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_list = [\n",
    "    'CPU time spent (ms)', \n",
    "    'Total time spent by all map tasks (ms)',\n",
    "    'Total time spent by all reduce tasks (ms)',\n",
    "    'GC time elapsed (ms)',\n",
    "    'Peak Map Physical memory (bytes)',\n",
    "\t'Peak Map Virtual memory (bytes)',\n",
    "\t'Peak Reduce Physical memory (bytes)',\n",
    "\t'Peak Reduce Virtual memory (bytes)'\n",
    "    ]\n",
    "dim = ['10KB', '1MB', '100MB', '1GB']\n",
    "\n",
    "for params in parameters_list:\n",
    "    freq_comb_df = pd.DataFrame(index=range(1, n_reducers+1), columns=dim)\n",
    "    freq_inmap_df = pd.DataFrame(index=range(1, n_reducers+1), columns=dim)\n",
    "    count_df = pd.DataFrame(index=methods, columns=dim)\n",
    "    count_df = count_df.fillna(0)\n",
    "    for method in methods:\n",
    "        for i in range(1, n_reducers+1):\n",
    "            for dim_directory in os.listdir(f'../resources/performance_analysis/output_{run_id}_{method}_{i}'):\n",
    "                log_file = f'../resources/performance_analysis/output_{run_id}_{method}_{i}/{dim_directory}/log.txt'\n",
    "                is_total_count = True\n",
    "                with open(log_file, 'r') as f:\n",
    "                    for line in f:\n",
    "                        if params in line:\n",
    "                            if is_total_count:\n",
    "                                count_df.loc[method, dim_directory] += float(line.split('=')[1])\n",
    "                                is_total_count = False\n",
    "                            else:\n",
    "                                if method == 'combiner':\n",
    "                                    freq_comb_df.loc[i, dim_directory] = float(line.split('=')[1])\n",
    "                                else:\n",
    "                                    freq_inmap_df.loc[i, dim_directory] = float(line.split('=')[1])\n",
    "                                is_total_count = True           \n",
    "    count_df = count_df / n_reducers\n",
    "\n",
    "    ax1 = freq_comb_df.plot(kind='bar', title='Letter frequency with combiner', figsize=(10, 5), xlabel=\"Number of reducers\", ylabel=f'{params}', rot=0)\n",
    "    ax1.yaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "    ax1.ticklabel_format(style='sci', scilimits=(0,0), axis='y')\n",
    "    ax1.set_yscale('log')\n",
    "    for p in ax1.patches:\n",
    "        ax1.annotate(f'{p.get_height():.0f}', (p.get_x() + p.get_width() / 2, p.get_height()), ha='center', va='bottom')\n",
    "\n",
    "    \n",
    "    ax2 = freq_inmap_df.plot(kind='bar', title='Letter frequency with inMapper combiner', figsize=(10, 5), xlabel=\"Number of reducers\", ylabel=f'{params}', rot=0)\n",
    "    ax2.yaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "    ax2.ticklabel_format(style='sci', scilimits=(0,0), axis='y')\n",
    "    ax2.set_yscale('log')\n",
    "    for p in ax2.patches:\n",
    "        ax2.annotate(f'{p.get_height():.0f}', (p.get_x() + p.get_width() / 2, p.get_height()), ha='center', va='bottom')\n",
    "\n",
    "    ax3 = count_df.plot(kind='bar', title='Letter total count', figsize=(10, 5), xlabel=\"Method\", ylabel=f'{params}', rot=0)\n",
    "    ax3.set_yscale('log')\n",
    "    for p in ax3.patches:\n",
    "        ax3.annotate(f'{p.get_height():.0f}', (p.get_x() + p.get_width() / 2, p.get_height()), ha='center', va='bottom')\n",
    "\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
